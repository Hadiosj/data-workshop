{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Workshop, Strasbourg 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop consists of different sections:\n",
    "- [Scraping](##Scraping)\n",
    "- [Visualization](##Visualization)\n",
    "- [Preprocessing](##Preprocessing)\n",
    "- [Predicting](##Predicting)\n",
    "\n",
    "In each section, you will explore the code and fill the missing code when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OvKzXnCFM3_"
   },
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1c08_fkEkrE"
   },
   "source": [
    "This first section will describe the process of scraping data from the [understat.com](https://understat.com) website, that has a lot of statistical information about all football games in top 5 European football leagues.\n",
    "\n",
    "The main information provided in the [understat.com](https://understat.com) home page is **Expected goals (xG)**, a new revolutionary football metric, which allows you to evaluate team and player performance. It is a statistical measure of the quality of chances created and conceded. \n",
    "\n",
    "Higher (xG) -> Expected to score more goals\n",
    "\n",
    "\n",
    "The website not only shows the (xG) metric, but much more, which makes this site perfect for scraping statistical data about football games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by installing the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 pandas numpy plotly ipywidgets seaborn matplotlib adjustText scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "WAoZi6Mlg121"
   },
   "outputs": [],
   "source": [
    "# Scraping\n",
    "import requests # Used for HTTP requests\n",
    "from bs4 import BeautifulSoup # Used for HTML parsing\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # Used for Data manipulation\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "\n",
    "# Data analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imMT_rB3Gj88"
   },
   "source": [
    "### Website research and structure of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the home page we can notice that the site has data for 6 European Leagues:\n",
    "*   La Liga\n",
    "*   EPL\n",
    "*   BundesLiga\n",
    "*   Serie A\n",
    "*   Ligue 1\n",
    "*   RFPL\n",
    "\n",
    "![leagues.jpg](https://github.com/Hadiosj/data-workshop/blob/main/data1.png?raw=true)\n",
    "\n",
    "<br>\n",
    "\n",
    "We can also notice that the data collected is starting from the 2014/2015 season:\n",
    "\n",
    "![seasons.jpg](https://github.com/Hadiosj/data-workshop/blob/main/data2.png?raw=true)\n",
    "\n",
    "<br>\n",
    "\n",
    "Another notion we make is the structure of URL. It is `https://understat.com/league` + `/name_of_the_league` + `/year_start_of_the_season`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "5GdbwW7Tg127"
   },
   "outputs": [],
   "source": [
    "# possible urls for all seasons of all leagues\n",
    "base_url = 'https://understat.com/league'\n",
    "leagues = ['La_liga', 'EPL', 'Bundesliga', 'Serie_A', 'Ligue_1', 'RFPL']\n",
    "seasons = ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023' ,'2024']\n",
    "\n",
    "# Create a URL based on your choice of league and season\n",
    "# For example, to get the EPL 2023 season data we should use:\n",
    "url = \"https://understat.com/league/EPL/2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to see if your URL is valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if not url.startswith('https://understat.com/'):\n",
    "        print(\"❌ URL must start with 'https://understat.com/'\")\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✅ URL is valid: {url}\")\n",
    "        else:\n",
    "            print(f\"❌ URL is invalid: {url}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ URL is invalid: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6TMbq27P5UP"
   },
   "source": [
    "Next step is to understand where the data is located on the web-page. For this we open Developer Tools in Chrome, go to tab \"Network\", find file with data (in this case 2018) and check the \"Response\" tab. This is what we will get after running *requests.get(URL)*\n",
    "\n",
    "![requests_response_1.jpg](http://sergilehkyi.com/wp-content/uploads/2019/06/requests_response_1.jpg)\n",
    "\n",
    "After going through content of the web-page we find that the data is stored under \"script\" tag and it is JSON encoded. So we will need to find this tag, get JSON from it and convert it into Python readable data structure.\n",
    "\n",
    "![requests_response_2.jpg](http://sergilehkyi.com/wp-content/uploads/2019/06/requests_response_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXPittjCP4V6"
   },
   "outputs": [],
   "source": [
    "# Starting with latest data for Spanish league, because I'm a Barcelona fan\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "# Based on the structure of the webpage, I found that data is in the JSON variable, under <script> tags\n",
    "scripts = soup.find_all('script')\n",
    "\n",
    "# Check our <script> tags\n",
    "for el in scripts:\n",
    "  print('*'*50)\n",
    "  print(el.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZiH8okNTlS_"
   },
   "source": [
    "### Working with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfGeAu6zTuZh"
   },
   "source": [
    "We found that the data interesting to us is stored in `teamsData` variable, after creating a soup of html tags it becomes just a string, so we find that text and extract JSON from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkB-izz7aTtK"
   },
   "outputs": [],
   "source": [
    "string_with_json_obj = ''\n",
    "\n",
    "# Find data for teams\n",
    "for script in scripts:\n",
    "    if 'teamsData' in str(script):\n",
    "        string_with_json_obj = str(script).strip()\n",
    "\n",
    "# strip unnecessary symbols and get only JSON data\n",
    "ind_start = string_with_json_obj.index(\"('\")+2\n",
    "ind_end = string_with_json_obj.index(\"')\")\n",
    "json_data = string_with_json_obj[ind_start:ind_end]\n",
    "\n",
    "json_data = json_data.encode('utf8').decode('unicode_escape')\n",
    "data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_foRghUdURG2"
   },
   "source": [
    "Once we have gotten our JSON and cleaned it up we can convert it into Python dictionary and check how it looks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PIcc9QcaJcO"
   },
   "source": [
    "### Understanding data with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# Iterate over each team (in case there are more than one)\n",
    "for team_id, team_data in data.items():\n",
    "    for game in team_data['history']:\n",
    "        row = {'team_name': team_data['title']}\n",
    "        row.update(game)  # Merge game data\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df.sample(10) # check out the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Ensure numeric types\n",
    "df['xG'] = pd.to_numeric(df['xG'], errors='coerce')\n",
    "df['scored'] = pd.to_numeric(df['scored'], errors='coerce')\n",
    "\n",
    "# Function to plot for selected team\n",
    "def plot_team_xg(team):\n",
    "    team_df = df[df['team_name'] == team].copy()\n",
    "    team_df = team_df.sort_values('date')\n",
    "    team_df['xG_roll'] = team_df['xG'].rolling(window=5).mean()\n",
    "    team_df['scored_roll'] = team_df['scored'].rolling(window=5).mean()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=team_df['date'], y=team_df['xG_roll'],\n",
    "                             mode='lines', name='xG (Rolling Avg)'))\n",
    "    fig.add_trace(go.Scatter(x=team_df['date'], y=team_df['scored_roll'],\n",
    "                             mode='lines', name='Goals Scored (Rolling Avg)'))\n",
    "    fig.update_layout(title=f'{team}: Rolling Average xG vs Goals',\n",
    "                      xaxis_title='Date',\n",
    "                      yaxis_title='Goals',\n",
    "                      xaxis_tickformat='%b %Y',  # Short month-year format\n",
    "                      xaxis=dict(tickangle=45),\n",
    "                      yaxis=dict(range=[0, team_df['scored_roll'].max() + 1]),\n",
    "                      template='plotly_white')\n",
    "    fig.show()\n",
    "\n",
    "# Interactive dropdown for team\n",
    "teams = sorted(df['team_name'].unique())\n",
    "interact(plot_team_xg, team=teams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_summary = df.groupby('team_name')[['xG', 'scored']].mean().reset_index()\n",
    "\n",
    "sns.scatterplot(data=team_summary, x='xG', y='scored')\n",
    "\n",
    "# Create a list of text annotations\n",
    "texts = []\n",
    "for i, row in team_summary.iterrows():\n",
    "    texts.append(plt.text(row['xG'], row['scored'], row['team_name'], fontsize=9))\n",
    "\n",
    "# Adjust text to avoid overlaps\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n",
    "\n",
    "plt.plot([team_summary['xG'].min(), team_summary['xG'].max()],\n",
    "         [team_summary['xG'].min(), team_summary['xG'].max()], 'r--')\n",
    "plt.title('Avg xG vs Avg Goals Scored per Team')\n",
    "plt.xlabel('Average xG')\n",
    "plt.ylabel('Average Goals Scored')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what can we use this data for?\n",
    "\n",
    "We want to answer the following:\n",
    "\n",
    "> **\"Does xG correlate with success? Is it a good indicator of performance?\"**\n",
    "\n",
    "By using **xG**, **xGA**, **non-penalty xG**, and similar stats to classify match outcomes (Win, Draw, Loss), we are testing:\n",
    "\n",
    "- Whether xG-based stats explain what happened in the match\n",
    "- If xG is a better metric for performance analysis than just looking at goals scored\n",
    "- If it's useful to analysts, coaches, or scouts as a post-match evaluation tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train a model to predict the number of goals scored, we need to preprocess our data. This is similar to what we'd do for classification, but our target variable will be different.\n",
    "\n",
    "Our main preprocessing tasks will be:\n",
    "1.  **Identify Target Variable**: Our target `y` will be the `scored` column, representing the actual number of goals scored by the team in a match.\n",
    "2.  **Handle Categorical Features**: We'll still need to convert categorical columns like `h_a` (home/away) into a numerical format as it can influence goal scoring.\n",
    "3.  **Feature Selection**: We'll choose features that we believe will be most predictive of the number of goals scored.\n",
    "4.  **Data Cleaning**: Ensure all selected features and the target variable are numeric and handle any missing or infinite values.\n",
    "5.  **Splitting Data**: We'll divide our dataset into a training set and a testing set.\n",
    "6.  **Feature Scaling**: Scale numerical features to help the regression model perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "df_regr = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify Target Variable & 4. Data Cleaning (Target)\n",
    "# Ensure 'scored' is numeric and handle missing/infinite values\n",
    "df_regr['scored'] = pd.to_numeric(df_regr['scored'], errors='coerce')\n",
    "df_regr.dropna(subset=['scored'], inplace=True) # Drop rows where 'scored' is NaN after coercion\n",
    "df_regr['scored'] = df_regr['scored'].astype(int) # Goals are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Handle Categorical Features\n",
    "df_regr['h_a'] = df_regr['h_a'].apply(lambda x: 1 if x == 'h' else 0)\n",
    "\n",
    "# 4. Data Cleaning (Features)\n",
    "# Define potential features and ensure they are clean\n",
    "feature_cols = ['h_a', 'xG', 'xGA', 'npxG', 'npxGA', 'deep', 'deep_allowed', 'xpts', 'pts', 'npxGD']\n",
    "df_regr.dropna(subset=feature_cols, inplace=True) # Drop rows with NaNs in these feature columns for simplicity\n",
    "\n",
    "for col in feature_cols:\n",
    "    if col != 'h_a': # 'h_a' is already 0 or 1\n",
    "        df_regr[col] = pd.to_numeric(df_regr[col], errors='coerce')\n",
    "        df_regr[col] = df_regr[col].fillna(df_regr[col].mean())\n",
    "        df_regr[col] = df_regr[col].replace([np.inf, -np.inf], df_regr[col].mean()) \n",
    "\n",
    "# 3. Feature Selection\n",
    "# We'll select features that might influence the number of goals scored.\n",
    "features_for_regression = ['h_a', 'xG', 'xGA', 'npxG', 'npxGA', 'deep', 'deep_allowed']\n",
    "X_regr = df_regr[features_for_regression]\n",
    "y_regr = df_regr['scored']\n",
    "\n",
    "# 5. Splitting Data\n",
    "X_train_regr, X_test_regr, y_train_regr, y_test_regr = train_test_split(X_regr, y_regr, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Feature Scaling\n",
    "scaler_regr = StandardScaler()\n",
    "X_train_regr_scaled = scaler_regr.fit_transform(X_train_regr)\n",
    "X_test_regr_scaled = scaler_regr.transform(X_test_regr)\n",
    "\n",
    "X_train_regr_scaled_df = pd.DataFrame(X_train_regr_scaled, columns=features_for_regression)\n",
    "X_test_regr_scaled_df = pd.DataFrame(X_test_regr_scaled, columns=features_for_regression)\n",
    "\n",
    "print(f\"Shape of X_train_regr: {X_train_regr_scaled_df.shape}\")\n",
    "print(f\"Shape of X_test_regr: {X_test_regr_scaled_df.shape}\")\n",
    "print(f\"Shape of y_train_regr: {y_train_regr.shape}\")\n",
    "print(f\"Shape of y_test_regr: {y_test_regr.shape}\")\n",
    "\n",
    "print(\"\\nPreview of preprocessed data for regression (first 5 rows of X_train_regr_scaled_df):\")\n",
    "X_train_regr_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll train a model to predict the actual number of goals a team scores in a match. This is a **regression problem** because the output (number of goals) is a continuous numerical value (though in practice, goals are discrete integers).\n",
    "\n",
    "We'll use a **Random Forest Regressor** for this task. It's a versatile model that can capture non-linear relationships in the data.\n",
    "\n",
    "Steps:\n",
    "1.  **Initialize and Train the Regression Model**: We'll use `X_train_regr_scaled` and `y_train_regr`.\n",
    "2.  **Make Predictions**: Predict the number of goals for the test data `X_test_regr_scaled`.\n",
    "3.  **Evaluate Performance**: We'll use regression metrics:\n",
    "    * **Mean Absolute Error (MAE)**: The average absolute difference between predicted and actual goals.\n",
    "    * **Mean Squared Error (MSE)**: The average of the squares of the errors. Penalizes larger errors more.\n",
    "    * **R-squared (R²)**: The proportion of the variance in the dependent variable that is predictable from the independent variables. Ranges from 0 to 1 (or can be negative for poor models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize and Train the Regression Model\n",
    "regressor_model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_leaf=5)\n",
    "regressor_model.fit(X_train_regr_scaled, y_train_regr)\n",
    "\n",
    "print(\"Regression model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our regressor performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Make Predictions\n",
    "y_pred_regr = regressor_model.predict(X_test_regr_scaled)\n",
    "\n",
    "# Since goals are integers, we can round the predictions (optional, but makes sense for this context)\n",
    "y_pred_regr_rounded = np.round(y_pred_regr).astype(int)\n",
    "\n",
    "# 3. Evaluate Performance\n",
    "mae = mean_absolute_error(y_test_regr, y_pred_regr)\n",
    "mse = mean_squared_error(y_test_regr, y_pred_regr)\n",
    "r2 = r2_score(y_test_regr, y_pred_regr)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f} goals\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\\n\")\n",
    "\n",
    "print(\"Evaluating with rounded predictions (closer to real-world goal counts):\")\n",
    "mae_rounded = mean_absolute_error(y_test_regr, y_pred_regr_rounded)\n",
    "mse_rounded = mean_squared_error(y_test_regr, y_pred_regr_rounded)\n",
    "r2_rounded = r2_score(y_test_regr, y_pred_regr_rounded)\n",
    "\n",
    "print(f\"MAE (rounded): {mae_rounded:.4f} goals\")\n",
    "print(f\"MSE (rounded): {mse_rounded:.4f}\")\n",
    "print(f\"R² (rounded): {r2_rounded:.4f}\")\n",
    "\n",
    "# Visualize actual vs. predicted goals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_regr, y_pred_regr, alpha=0.5, label='Predicted (Raw)')\n",
    "plt.scatter(y_test_regr, y_pred_regr_rounded, alpha=0.5, color='red', marker='x', label='Predicted (Rounded)')\n",
    "plt.plot([y_test_regr.min(), y_test_regr.max()], [y_test_regr.min(), y_test_regr.max()], 'k--', lw=2, label='Ideal Prediction')\n",
    "plt.xlabel('Actual Goals Scored')\n",
    "plt.ylabel('Predicted Goals Scored')\n",
    "plt.title('Actual vs. Predicted Goals Scored')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_test_regr is a NumPy array or Pandas Series for easy manipulation\n",
    "if not isinstance(y_test_regr, (np.ndarray, pd.Series)):\n",
    "    y_test_regr_np = np.array(y_test_regr)\n",
    "else:\n",
    "    y_test_regr_np = y_test_regr.to_numpy() if isinstance(y_test_regr, pd.Series) else y_test_regr\n",
    "\n",
    "if not isinstance(y_pred_regr, (np.ndarray, pd.Series)):\n",
    "    y_pred_regr_np = np.array(y_pred_regr)\n",
    "else:\n",
    "    y_pred_regr_np = y_pred_regr.to_numpy() if isinstance(y_pred_regr, pd.Series) else y_pred_regr\n",
    "    \n",
    "if not isinstance(y_pred_regr_rounded, (np.ndarray, pd.Series)):\n",
    "    y_pred_regr_rounded_np = np.array(y_pred_regr_rounded)\n",
    "else:\n",
    "    y_pred_regr_rounded_np = y_pred_regr_rounded.to_numpy() if isinstance(y_pred_regr_rounded, pd.Series) else y_pred_regr_rounded\n",
    "\n",
    "\n",
    "# Residuals are the differences between actual and predicted values\n",
    "residuals = y_test_regr_np - y_pred_regr_np # Using raw predictions for residuals\n",
    "\n",
    "\n",
    "# --- 3. Error Metrics by Actual Goals (using rounded predictions for more intuitive MAE) ---\n",
    "# Create a DataFrame for easier analysis\n",
    "results_df = pd.DataFrame({'actual_goals': y_test_regr_np, 'predicted_goals_rounded': y_pred_regr_rounded_np})\n",
    "results_df['absolute_error'] = abs(results_df['actual_goals'] - results_df['predicted_goals_rounded'])\n",
    "\n",
    "mae_per_goal_count = results_df.groupby('actual_goals')['absolute_error'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "mae_per_goal_count.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Actual Number of Goals Scored')\n",
    "plt.ylabel('Mean Absolute Error (Rounded Predictions)')\n",
    "plt.title('MAE per Actual Goal Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "Interpretation of Mean Predicted vs. Actual Goals:\n",
    "- This plot helps to see if there's a systematic over- or under-prediction for certain goal counts.\n",
    "- If the orange bars (mean predicted) are consistently below the gray bars (actual) for higher goal counts,\n",
    "  it means the model tends to under-predict high-scoring games. The opposite for over-prediction.\n",
    "- The closer the orange bars are to the ideal prediction line (and thus the gray bars), the better calibrated the model is on average for each goal category.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# --- 5. Clearer Scatter Plot: Seaborn's regplot or a Hexbin Plot ---\n",
    "\n",
    "# Using regplot for a trend line (can be slow with many points, so let's use a sample if too large)\n",
    "sample_size = min(len(y_test_regr_np), 1000) # Use at most 1000 points for regplot\n",
    "indices = np.random.choice(len(y_test_regr_np), sample_size, replace=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x=y_test_regr_np[indices], y=y_pred_regr_np[indices], scatter_kws={'alpha':0.3}, line_kws={'color':'red'})\n",
    "plt.plot([y_test_regr_np.min(), y_test_regr_np.max()], [y_test_regr_np.min(), y_test_regr_np.max()], 'k--', lw=2, label='Ideal Prediction')\n",
    "plt.xlabel('Actual Goals Scored')\n",
    "plt.ylabel('Predicted Goals Scored (Raw)')\n",
    "plt.title('Actual vs. Predicted Goals with Regression Line (Sampled)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "Interpretation of Clearer Scatter Plots:\n",
    "- `regplot`: Shows the general trend. If the red regression line is close to the black dashed 'Ideal Prediction' line,\n",
    "  it indicates a good linear relationship. The spread of points around the line shows the variance.\n",
    "- `hexbin plot`: Useful for very dense scatter plots. Darker hexagons mean more data points fall into that\n",
    "  actual/predicted goal combination. Ideally, the darkest hexagons would cluster along the 'Ideal Prediction' line.\n",
    "  This helps see where the bulk of predictions lie.\n",
    "\"\"\")\n",
    "\n",
    "# --- Overall Summary of Regression Performance ---\n",
    "print(\"\\\\n--- Overall Summary of Regression Performance ---\")\n",
    "print(f\"R-squared (R²): {r2_score(y_test_regr_np, y_pred_regr_np):.4f} (Explains {r2_score(y_test_regr_np, y_pred_regr_np)*100:.2f}% of variance in goals scored using raw predictions)\")\n",
    "print(f\"Mean Absolute Error (MAE - Raw): {mean_absolute_error(y_test_regr_np, y_pred_regr_np):.4f} goals (On average, raw predictions are off by this many goals)\")\n",
    "print(f\"Mean Absolute Error (MAE - Rounded): {mean_absolute_error(y_test_regr_np, y_pred_regr_rounded_np):.4f} goals (On average, rounded predictions are off by this many goals)\")\n",
    "\n",
    "print(\"\"\"\n",
    "Final Thoughts on Model Performance:\n",
    "- Predicting exact goal counts in football is notoriously difficult due to the sport's inherent randomness and low-scoring nature.\n",
    "- An R² value significantly above 0 indicates the model has some predictive power beyond just guessing the average. For example, an R² of 0.3-0.5 in this context might be considered reasonable.\n",
    "- The MAE (especially the rounded one) gives a practical sense of the prediction error. If actual goals are typically 0-3, an MAE around 0.7-1.0 might be typical for such models.\n",
    "- The plots help identify if the model has systematic biases (e.g., always underpredicting high scores) or if errors are fairly random.\n",
    "- The xG-based features likely provide a good baseline expectation, but the model will struggle with 'upsets' or unusually high/low scoring games not well-reflected in the xG for that specific match.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Regression Results\n",
    "\n",
    "Now we are trying to predict the *exact number of goals scored*, which is a more challenging task than predicting just Win/Draw/Loss.\n",
    "\n",
    "-   **MAE**: This tells us, on average, how far off our predictions are from the actual number of goals. An MAE of 0.8 would mean our predictions are off by about 0.8 goals on average.\n",
    "-   **MSE**: This metric penalizes larger errors more heavily. It's harder to interpret directly in terms of goals but is useful for comparing models.\n",
    "-   **R²**: This indicates how much of the variability in actual goals scored is explained by our model. An R² of 0.4 means our model explains 40% of the variance in goals scored. Higher is better (closer to 1).\n",
    "\n",
    "The scatter plot helps visualize the model's performance. Ideally, points would lie perfectly on the dashed diagonal line.\n",
    "\n",
    "**Does xG correlate with the number of goals scored?**\n",
    "If the R² value is reasonably above 0 and the MAE is not excessively high (considering typical goal counts in matches), it suggests that the xG-related features do provide some information for predicting the actual number of goals. However, football is a highly variable sport, and predicting exact goal counts is difficult. There's a lot of randomness (luck, individual brilliance/errors) that xG alone cannot capture.\n",
    "\n",
    "The model gives us an idea of *expected* performance translating into *actual* goals, but the inherent unpredictability of football will always limit the accuracy of such predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
